# -*- coding: utf-8 -*-
"""recommendation-system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n42GQLsUPjam0LQ6342cWX7wTjRZHGI_

# Import Library yang Digunakan
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import linear_kernel

from tensorflow import keras
from tensorflow.keras import layers

"""# Load kaggle api"""

# Upload kaggle.json
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""# Data Understanding"""

!kaggle datasets download -d nicoletacilibiu/movies-and-ratings-for-recommendation-system -p /kaggle

!unzip /kaggle/movies-and-ratings-for-recommendation-system.zip -d /kaggle

# konversi data movies ke dalam dataframe
movies_df = pd.read_csv('/kaggle/movies.csv', on_bad_lines='skip')
movies_df.head()

# konversi data ratings ke dalam dataframe
ratings_df = pd.read_csv('/kaggle/ratings.csv', on_bad_lines='skip')
ratings_df.head()

# cek jumlah data
print('Jumlah data movies: ',len(movies_df.movieId.unique()))
print('Jumlah data user: ',len(ratings_df.userId.unique()))

"""# Univariate Exploratory Data Analysis

## Deskripsi Variabel
"""

movies_df.info()

"""insight :
- terdapat 9742 baris data di dalam dataset
- data terdiri dari 3 kolom yaitu : movieId, title, dan genres.
- terdapat 1 data bertipe int64 yaitu kolom movieId, dan 2 kolom bertipe object yaitu kolom title dan genres
"""

ratings_df.info()

"""insight :
- terdapat 100836 baris data di dalam dataset.
- data terdiri dari 4 kolom yaitu : userId, movieId, rating, dan timestamp.
- terdapat 3 kolom bertipe int64, yaitu kolom userId, movieId, dan timestamp. terdapat 1 kolom bertipe float yaitu kolom rating.

## Memeriksa Missing Value
"""

missing_values_movie = movies_df.isnull().sum()
print('jumlah missing value pada data movie: ', missing_values_movie)

missing_values_rating = ratings_df.isnull().sum()
print('jumlah missing value pada data rating: ', missing_values_rating)

"""## Memeriksa Outlier"""

sns.set(style="whitegrid")

# Buat figure boxplot
plt.figure(figsize=(8, 5))
sns.boxplot(x=ratings_df['rating'], color='skyblue')

# Judul dan label
plt.title('Distribusi Rating Film (Boxplot)', fontsize=14)
plt.xlabel('Rating')

# Tampilkan plot
plt.show()

"""## Memeriksa data duplikat"""

# jumlah data duplikat pada data movie
jumlah_duplikat_movie = movies_df.duplicated().sum()
print('Jumlah data duplikat pada data movie: ', jumlah_duplikat_movie)

# jumlah data duplikat pada data rating
jumlah_duplikat_rating = ratings_df.duplicated().sum()
print('Jumlah data duplikat pada data rating', jumlah_duplikat_rating)

"""## Univariate Analysis"""

# cek daftar genre
all_genres = movies_df['genres'].str.split('|').explode()

unique_genres = all_genres.unique()
print('Jumlah genre unik:', len(unique_genres))
print('Daftar genre unik:')
for genre in unique_genres:
    print('-', genre)

# Visualisasi jumlah genre
genre_counts = all_genres.value_counts()

# Tampilkan  genre terbanyak
print("Genre terbanyak:\n")
print(genre_counts)
# Visualisasi dengan barplot
plt.figure(figsize=(12, 6))
sns.barplot(x=genre_counts.values, y=genre_counts.index, palette='viridis')

plt.title('Genre dalam Dataset')
plt.xlabel('Jumlah Film')
plt.ylabel('Genre')
plt.tight_layout()
plt.show()

# visualisasi persebaran ratings
sns.set(style="whitegrid")

# Ukuran figure
plt.figure(figsize=(8, 5))

# Plot distribusi rating
sns.countplot(x='rating', data=ratings_df, palette='viridis')

# Tambahkan judul dan label
plt.title('Distribusi Rating Film')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.xticks(rotation=45)  # Rotasi label jika perlu

# Tampilkan plot
plt.tight_layout()
plt.show()

print('Jumlah userId: ', len(ratings_df.userId.unique()))
print('Jumlah movieId: ', len(ratings_df.movieId.unique()))
print('Jumlah data ratings: ', len(ratings_df))

"""# Data Preprocessing

## Data movie
"""

# membuat salinan dari data movies_df
fix_movie = movies_df.copy()

# mengonversi setiap data series 'movieId' menjadi bentuk list
moviesId = fix_movie['movieId'].tolist()

# mengonversi data 'title' menjadi bentuk list
title = fix_movie['title'].tolist()

# mengonversi data 'genres' menjadi bentuk list
genres = fix_movie['genres'].tolist()

print(len(moviesId))
print(len(title))
print(len(genres))

# Membuat dictionary untuk data ‘moviesId’, ‘title’, dan ‘genres’
movies_new = pd.DataFrame({
    'moviesId': moviesId,
    'title': title,
    'genres': genres
})
movies_new

"""## Data rating"""

ratings_new = ratings_df.drop(columns=['timestamp'])
ratings_new.head()

# encoding kolom userId
userIds = ratings_new['userId'].unique().tolist()
print('list userIds:', userIds)

user_to_user_encoded = {x: i for i, x in enumerate(userIds)}
print('encoded userIds : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(userIds)}
print('encoded angka ke userIds: ', user_encoded_to_user)

# encoding kolom movieId
movieIds = ratings_new['movieId'].unique().tolist()
print('list movieIds:', movieIds)

movie_to_movie_encoded = {x: i for i, x in enumerate(movieIds)}
print('encoded movieID : ', movie_to_movie_encoded)

movie_encoded_to_movie = {i: x for i, x in enumerate(movieIds)}
print('encoded angka ke movieID: ', movie_encoded_to_movie)

# menentukan jumlah user di dalam data ratings
num_users = len(userIds)
print('jumlah user:', num_users)

# menentukan jumlah film di dalam data ratings
num_movies = len(movieIds)
print('jumlah movie:', num_movies)

ratings_df['rating'] = ratings_df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(ratings_new['rating'])
max_rating = max(ratings_new['rating'])

print('Number of User: {}, Number of Movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movies, min_rating, max_rating
))

# mengacak kolom ratings
ratings_new = ratings_new.sample(frac=1, random_state=42)
ratings_new

# Mapping userID ke dataframe user
ratings_new['user'] = ratings_new['userId'].map(user_to_user_encoded)

# Mapping movieId ke dataframe movie
ratings_new['movie'] = ratings_new['movieId'].map(movie_to_movie_encoded)

X = ratings_new[['user', 'movie']]
y = ratings_new['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * ratings_new.shape[0])
X_train, X_val, y_train, y_val = (
    X[:train_indices],
    X[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(X, y)

"""# Model Deployment

## Model Content Based Filtering
"""

# membuat salinan dari data movies
data_movies = movies_df

"""### TF_IDF Vectorizer"""

# Inisialisasi TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')

tfidf_matrix = tfidf.fit_transform(data_movies['genres'])

tfidf.get_feature_names_out()

tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=data_movies['title']
).sample(22, axis=1).sample(10, axis=0)

"""### Cek Cosine Similarity"""

# menggunakan subset 1000 film pertama untuk menghemat ram
tfidf_sample = tfidf_matrix[:1000]
cosine_sim_sample = cosine_similarity(tfidf_sample)
cosine_sim_sample

cosine_sim_df = pd.DataFrame(
    cosine_sim_sample,
    index=data_movies['title'][:1000],
    columns=data_movies['title'][:1000]
)

print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

# Fungsi rekomendasi film berdasarkan cosine similarity
def movie_recommendations(title, similarity_data=cosine_sim_df, items=data_movies[['title', 'genres']], k=5):
    index = similarity_data.loc[:, title].to_numpy().argpartition(range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(title, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

data_movies[data_movies.title.eq('Jumanji (1995)')]

movie_recommendations('Jumanji (1995)')

"""## Model Collaborative Filtering"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_resto, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_resto = num_resto
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.resto_embedding = layers.Embedding( # layer embeddings resto
        num_resto,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_resto, 1) # layer embedding resto bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    resto_vector = self.resto_embedding(inputs[:, 1]) # memanggil layer embedding 3
    resto_bias = self.resto_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2)

    x = dot_user_resto + user_bias + resto_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_movies, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (X_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Mengambil satu sample user secara acak
user_id = ratings_df['userId'].sample(1).iloc[0]

# mengambil film yang sudah dirating oleh user ini
movies_watched_by_user = ratings_df[ratings_df.userId == user_id]

# mengambil film yang belum ditonton oleh user ini
movies_not_watched = movies_df[~movies_df['movieId'].isin(movies_watched_by_user.movieId.values)]

# Hanya ambil movieId yang ada di kamus encoding
movies_not_watched = list(
    set(movies_not_watched.movieId.values)
    .intersection(set(movie_to_movie_encoded.keys()))
)

# Encoding movieId
movies_not_watched_encoded = [[movie_to_movie_encoded.get(x)] for x in movies_not_watched]

# Encoding userId
user_encoded = user_to_user_encoded.get(user_id)

# membuat array pasangan user dan semua film yang belum ditonton
user_movie_array = np.hstack(
    (
        np.array([[user_encoded]] * len(movies_not_watched_encoded)),
        np.array(movies_not_watched_encoded)
    )
)

ratings = model.predict(user_movie_array).flatten()

# Ambil indeks top-N prediksi tertinggi
top_indices = ratings.argsort()[-10:][::-1]  # top 10

# Ambil movieId-nya kembali dari encoded
recommended_movie_ids = [
    movie_encoded_to_movie[i[0]] for i in np.array(movies_not_watched_encoded)[top_indices]
]

# Tampilkan judul film rekomendasi
recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)]
print(recommended_movies[['movieId', 'title','genres']])